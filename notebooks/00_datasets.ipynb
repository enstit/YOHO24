{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOHO Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to locally download the datasets used in the [the original YOHO paper](https://doi.org/10.48550/arXiv.2109.00962), meaning:\n",
    "\n",
    "* the _MuSpeak dataset_,\n",
    "* the _TUT Sound Event Detection dataset_ and\n",
    "* the _Urban Sound Event Detection dataset_.\n",
    "\n",
    "We defined useful funcitons in the `utils` package to make easier the process of data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules before entering the execution of code typed at \n",
    "# the IPython prompt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as audio_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUT Sound Events Detection\n",
    "\n",
    "The dataset is downloaded from the [DCASE2017 Challenge official website](https://dcase.community/challenge2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9969401it [13:30, 12292.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://zenodo.org/api/records/814831/files-archive to ../data/tut.zip\n",
      "Extracted ../data/tut.zip to ../data/tut/TUT-sound-events-2017-development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.meta.zip to ../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:07<00:07,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.audio.1.zip to ../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.audio.1\n",
      "Extracted ../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.doc.zip to ../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.audio.2.zip to ../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.audio.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3033157it [02:08, 23606.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://zenodo.org/api/records/1040179/files-archive to ../data/tut.zip\n",
      "Extracted ../data/tut.zip to ../data/tut/TUT-sound-events-2017-evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ../data/tut/TUT-sound-events-2017-evaluation/TUT-sound-events-2017-evaluation.doc.zip to ../data/tut/TUT-sound-events-2017-evaluation/TUT-sound-events-2017-evaluation.doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ../data/tut/TUT-sound-events-2017-evaluation/TUT-sound-events-2017-evaluation.audio.zip to ../data/tut/TUT-sound-events-2017-evaluation/TUT-sound-events-2017-evaluation.audio\n",
      "Extracted ../data/tut/TUT-sound-events-2017-evaluation/TUT-sound-events-2017-evaluation.meta.zip to ../data/tut/TUT-sound-events-2017-evaluation/TUT-sound-events-2017-evaluation.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TUT Sound Events 2017 Dataset\n",
    "tut_urls = [(\"TUT-sound-events-2017-development\", \"https://zenodo.org/api/records/814831/files-archive\"),\n",
    "            (\"TUT-sound-events-2017-evaluation\", \"https://zenodo.org/api/records/1040179/files-archive\")\n",
    "            ]\n",
    "tut_zip_path = \"../data/tut.zip\"\n",
    "tut_extract_to = \"../data/tut\"\n",
    "\n",
    "for tut_name, tut_url in tut_urls:\n",
    "    tut_extract_to_subfolder = tut_extract_to + '/' + tut_name\n",
    "    audio_utils.download_file(tut_url, tut_zip_path)\n",
    "    audio_utils.uncompress_file(tut_zip_path, tut_extract_to_subfolder)\n",
    "\n",
    "    for item in tqdm(os.listdir(tut_extract_to_subfolder)):\n",
    "        if item.endswith('.zip'):\n",
    "            zipped_file = tut_extract_to_subfolder + '/' + item\n",
    "            unzipped_file = zipped_file.rsplit(\n",
    "                \".\", 1)[0]  # Remove .zip extension\n",
    "            audio_utils.uncompress_file(zipped_file, unzipped_file)\n",
    "            #os.remove(zipped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Urban-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urban-SED Dataset\n",
    "urbansed_url = \"https://zenodo.org/api/records/1324404/files-archive\"\n",
    "urbansed_zip_path = \"../data/urbansed.zip\"\n",
    "urbansed_extract_to = \"../data/urbansed\"\n",
    "\n",
    "audio_utils.download_file(urbansed_url, urbansed_zip_path)\n",
    "audio_utils.uncompress_file(urbansed_zip_path, urbansed_extract_to)\n",
    "# The folder contains a compressed subfolder\n",
    "audio_utils.uncompress_file(urbansed_extract_to + \"/URBAN-SED_v2.0.0.tar.gz\", urbansed_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Process Annotations**: Read the annotation files for each dataset and convert the event data into a unified format. For example, for the MUSP dataset, annotations are transformed into a list of events where each event is represented as a tuple containing the event type ('m' for music or 's' for speech), start time, and end time.\n",
    "\n",
    "2. **Save Processed Data**: Save the processed data, including the audio file paths and their corresponding events, into a new CSV file. This structured data will serve as the input for data generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUT Sound Events Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_1_PATH = './data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.audio.1/TUT-sound-events-2017-development/audio/street/'\n",
    "AUDIO_2_PATH = './data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.audio.2/TUT-sound-events-2017-development/audio/street/'\n",
    "DATA_PATH = '../data/tut/'\n",
    "\n",
    "DEVELOPMENT_ANNOTATIONS_PATH = '../data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.meta/TUT-sound-events-2017-development/meta/street/'\n",
    "\n",
    "files = audio_utils.get_files(DEVELOPMENT_ANNOTATIONS_PATH, extensions='.ann')\n",
    "\n",
    "tut_data_train = {}\n",
    "for f in files:\n",
    "    with open(DEVELOPMENT_ANNOTATIONS_PATH + f, 'r'):\n",
    "\n",
    "        f_name = f.split('.')[0] + '.wav'\n",
    "\n",
    "        if f_name in ['a128.wav', 'a131.wav', 'b007.wav', 'b093.wav']:\n",
    "            f_path = AUDIO_2_PATH + f_name\n",
    "        else:\n",
    "          f_path = AUDIO_1_PATH + f_name\n",
    "          \n",
    "        tut_data_train[f_path] = []\n",
    "\n",
    "        print(f'Processing {f_path}...')\n",
    "\n",
    "        with open(DEVELOPMENT_ANNOTATIONS_PATH + f, 'r') as file:\n",
    "\n",
    "            reader = csv.reader(file)\n",
    "\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    # split in \\t and get the start and end time\n",
    "                    row = row[0].split('\\t')\n",
    "                    start = float(row[2])\n",
    "                    end = float(row[3])\n",
    "                    label = row[4]\n",
    "                    tut_data_train[f_path].append(\n",
    "                        (label, start, end)\n",
    "                    )\n",
    "\n",
    "audio_utils.write_data_to_csv(tut_data_train, '../data/tut.train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_1_PATH = './data/tut/TUT-sound-events-2017-evaluation/TUT-sound-events-2017-evaluation.audio/TUT-sound-events-2017-evaluation/audio/street/'\n",
    "DATA_PATH = '../data/tut/'\n",
    "\n",
    "EVALUATION_ANNOTATIONS_PATH = '../data/tut/TUT-sound-events-2017-evaluation/TUT-sound-events-2017-evaluation.meta/TUT-sound-events-2017-evaluation/meta/street/'\n",
    "\n",
    "files = audio_utils.get_files(EVALUATION_ANNOTATIONS_PATH, extensions='.ann')\n",
    "\n",
    "tut_data_evaluation = {}\n",
    "for f in files:\n",
    "    with open(EVALUATION_ANNOTATIONS_PATH + f, 'r'):\n",
    "\n",
    "        f_name = f.split('.')[0] + '.wav'\n",
    "        f_path = AUDIO_1_PATH + f_name\n",
    "          \n",
    "        tut_data_evaluation[f_path] = []\n",
    "\n",
    "        print(f'Processing {f_path}...')\n",
    "\n",
    "        with open(EVALUATION_ANNOTATIONS_PATH + f, 'r') as file:\n",
    "\n",
    "            reader = csv.reader(file)\n",
    "\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    # split in \\t and get the start and end time\n",
    "                    row = row[0].split('\\t')\n",
    "                    start = float(row[0])\n",
    "                    end = float(row[1])\n",
    "                    label = row[2]\n",
    "                    tut_data_evaluation[f_path].append(\n",
    "                        (label, start, end)\n",
    "                    )\n",
    "\n",
    "audio_utils.write_data_to_csv(tut_data_evaluation, '../data/tut.evaluation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Urban-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jams\n",
    "URBAN_SED_ANNOTATIONS_PATH = './data/urbansed/annotations/train/'\n",
    "\n",
    "files = audio_utils.get_files(URBAN_SED_ANNOTATIONS_PATH, extensions='jams')\n",
    "\n",
    "def parse_jams_file(jams_file):\n",
    "    \"\"\"\n",
    "    Parse a JAMS file and extract the annotations.\n",
    "\n",
    "    Parameters:\n",
    "    - jams_file (str): The path to the JAMS file.\n",
    "\n",
    "    Returns:\n",
    "    - events (list): A list of tuples containing the event type, start time, and end time.\n",
    "    \"\"\"\n",
    "    jam = jams.load(jams_file)\n",
    "    events = []\n",
    "    for annotation in jam.annotations:\n",
    "\n",
    "        for obs in annotation.data:\n",
    "\n",
    "            start_time = obs.value['event_time']\n",
    "            end_time = obs.value['event_time'] + obs.value['event_duration']\n",
    "            label = obs.value['label']\n",
    "            events.append((label, start_time, end_time))\n",
    "\n",
    "        return events\n",
    "\n",
    "DATA_PATH = './data/urbansed/annotations/train/'\n",
    "AUDIO_PATH = './data/urbansed/audio/train/'\n",
    "\n",
    "urbansed_data = {}\n",
    "for f in files:\n",
    "    f_path = DATA_PATH + f\n",
    "    f = f.split('.')[0] + '.wav'\n",
    "    urbansed_data[AUDIO_PATH+f] = parse_jams_file(f_path)\n",
    "\n",
    "audio_utils.write_data_to_csv(urbansed_data, './data/urbansed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
