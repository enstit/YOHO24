{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOHO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import used libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(f\"{pd.__name__} version: {pd.__version__}\")\n",
    "print(f\"{matplotlib.__name__} version: {matplotlib.__version__}\")\n",
    "print(f\"{librosa.__name__} version: {librosa.__version__}\")\n",
    "print(f\"{torch.__name__} version: {torch.__version__}\")\n",
    "\n",
    "\n",
    "from yoho24.utils import AudioClip, AudioFile, TUTDataset, YOHODataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_melspectrogram(\n",
    "    audio: AudioFile, n_mels: int = 40, win_len: float = 1.00, hop_len: float = 1.00\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the Mel spectrogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(f\"Mel spectrogram\")\n",
    "    librosa.display.specshow(\n",
    "        data=audio.mel_spectrogram(n_mels=n_mels, win_len=win_len, hop_len=hop_len), sr=audio.sr, x_axis=\"frames\", y_axis=\"mel\"\n",
    "    )\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MELS = 40\n",
    "WIN_S = 0.04 # 40 ms\n",
    "HOP_S = 0.01 # 10 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = [\n",
    "    AudioFile(filepath=file.filepath, labels=eval(file.events))\n",
    "    for _, file in pd.read_csv(\"./data/processed/TUT/TUT-sound-events-2017-development.csv\").iterrows()\n",
    "]\n",
    "\n",
    "audioclips = [\n",
    "    audioclip\n",
    "    for _, audio in enumerate(audios)\n",
    "    for audioclip in audio.subdivide(win_len=2.56, hop_len=1.96)\n",
    "    if _ < 1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioclips[5].plot_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios[1].play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MELS = 40\n",
    "HOP_MS = 10\n",
    "WIN_MS = 40\n",
    "\n",
    "tut_train = TUTDataset(\n",
    "    audios=audioclips,\n",
    ")\n",
    "\n",
    "print(f\"Number of audio files: {len(tut_train)}\")\n",
    "print(f\"Duration: {tut_train.audios[0].duration} seconds\")\n",
    "print(f\"Sampling rate: {tut_train.audios[0].sr} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = YOHODataGenerator(tut_train, batch_size=1, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mel = np.zeros((257, 40))\n",
    "mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "from models import YOHO\n",
    "\n",
    "prediction = YOHO(input_shape=(1, 257, 40), output_shape=(9,18))(mel)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(YOHO(input_shape=(1, 257, 40), output_shape=(18,9)), (1, 257, 40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
