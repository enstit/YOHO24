{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urbansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules before entering the execution of code typed at \n",
    "# the IPython prompt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import used libraries\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(f\"{pd.__name__} version: {pd.__version__}\")\n",
    "print(f\"{matplotlib.__name__} version: {matplotlib.__version__}\")\n",
    "print(f\"{librosa.__name__} version: {librosa.__version__}\")\n",
    "print(f\"{torch.__name__} version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yoho.utils import AudioFile\n",
    "\n",
    "def plot_melspectrogram(\n",
    "    audio: AudioFile,\n",
    "    n_mels: int = 40,\n",
    "    win_len: float = 0.04,\n",
    "    hop_len: float = 0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the Mel spectrogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(f\"Mel spectrogram\")\n",
    "    librosa.display.specshow(\n",
    "        data=audio.mel_spectrogram(\n",
    "            n_mels=n_mels, win_len=win_len, hop_len=hop_len\n",
    "        ),\n",
    "        sr=audio.sr,\n",
    "        hop_length=hop_len*audio.sr,\n",
    "        x_axis=\"s\",\n",
    "        y_axis=\"mel\",\n",
    "    )\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yoho.train import load_dataset\n",
    "from yoho.utils import UrbanSEDDataset\n",
    "\n",
    "urbansed_train: UrbanSEDDataset = load_dataset(partition=\"train\")\n",
    "urbansed_val: UrbanSEDDataset = load_dataset(partition=\"validate\")\n",
    "\n",
    "print(f\"Number of audio clips in the train dataset: {len(urbansed_train)}\")\n",
    "print(f\"Number of audio clips in the validation dataset: {len(urbansed_val)}\")\n",
    "print(f\"Duration of each audio clips: {urbansed_train.audios[0].duration} seconds\")\n",
    "print(f\"Sampling rate of each audio clips: {urbansed_train.audios[0].sr} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yoho.utils import YOHODataGenerator\n",
    "\n",
    "val_dataloader = YOHODataGenerator(\n",
    "    urbansed_val, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "val_features, val_labels = next(iter(val_dataloader))\n",
    "\n",
    "print(f\"Test features shape: {val_features.shape}\")\n",
    "print(f\"Test labels shape: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOHO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "from yoho.models import YOHO\n",
    "from yoho.train import get_device\n",
    "\n",
    "# Get the available device (cuda, mps or cpu)\n",
    "device = get_device()\n",
    "\n",
    "# Define the model\n",
    "urbansed_model = YOHO(\n",
    "    name=\"UrbanSEDYOHO\",\n",
    "    input_shape=(1, 40, 257), n_classes=len(urbansed_train.labels)\n",
    ").to(device)\n",
    "\n",
    "urbansed_model.load(f\"./models/{urbansed_model.name}_checkpoint.pth.tar\", device)\n",
    "\n",
    "summary(\n",
    "    urbansed_model,\n",
    "    input_size=(1, 40, 257),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the losses.json file:\n",
    "\n",
    "# Load the losses.json file\n",
    "with open(\"./models/UrbanSEDYOHO_losses.json\", \"r\") as f:\n",
    "    losses = json.load(f)\n",
    "\n",
    "# Extract epochs, train_loss, and val_loss\n",
    "epochs = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch, metrics in losses.items():\n",
    "    epochs.append(int(epoch))\n",
    "    train_losses.append(metrics[\"train_loss\"])\n",
    "    val_losses.append(metrics[\"val_loss\"])\n",
    "\n",
    "# Sort the data by epochs to ensure correct plotting\n",
    "epochs, train_losses, val_losses = zip(\n",
    "    *sorted(zip(epochs, train_losses, val_losses))\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\", linestyle=\"--\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Losses\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = urbansed_model(val_features.__getitem__(11).unsqueeze(0).to(device))\n",
    "target = urbansed_val._get_output(11)\n",
    "# Add the batch dimension to the target which is a numpy array\n",
    "target = torch.from_numpy(target).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels_ = [\n",
    "    \"noise\",\n",
    "    \"air_conditioner\",\n",
    "    \"car_horn\",\n",
    "    \"children_playing\",\n",
    "    \"dog_bark\",\n",
    "    \"drilling\",\n",
    "    \"engine_idling\",\n",
    "    \"gun_shot\",\n",
    "    \"jackhammer\",\n",
    "    \"siren\",\n",
    "    \"street_music\",\n",
    "]\n",
    "\n",
    "\n",
    "def process_output(output: np.array) -> list[tuple[str, float, float]]:\n",
    "\n",
    "    STEPS_NO = 9\n",
    "    step_duration = 2.56 / STEPS_NO\n",
    "    MIN_EVENT_DURATION = 0\n",
    "    MIN_SILENCE_DURATION = 1.0\n",
    "\n",
    "    labels = []\n",
    "    for i in range(output.shape[1]):\n",
    "\n",
    "        for j in range(0, output.shape[0], 3):\n",
    "            if output[j, i] >= 0.5:\n",
    "                label = labels_[j // 3]\n",
    "                start = (\n",
    "                    i * step_duration + output[j + 1, i].item() * step_duration\n",
    "                )\n",
    "                end = (\n",
    "                    i * step_duration + output[j + 2, i].item() * step_duration\n",
    "                )\n",
    "                labels.append((label, round(start, 2), round(end, 2)))\n",
    "\n",
    "    # Order the labels by class\n",
    "    labels = sorted(labels, key=lambda x: x[0])\n",
    "\n",
    "    # Merge events of the same class that are close to each other\n",
    "    merged_labels = []\n",
    "    for label, start, end in labels:\n",
    "        if not merged_labels:\n",
    "            merged_labels.append((label, start, end))\n",
    "        else:\n",
    "            prev_label, prev_start, prev_end = merged_labels[-1]\n",
    "            if prev_label == label and start - prev_end < MIN_SILENCE_DURATION:\n",
    "                merged_labels[-1] = (label, prev_start, end)\n",
    "            else:\n",
    "                merged_labels.append((label, start, end))\n",
    "\n",
    "    # Remove events that are too short\n",
    "    merged_labels = [\n",
    "        (label, start, end)\n",
    "        for label, start, end in merged_labels\n",
    "        if end - start >= MIN_EVENT_DURATION\n",
    "    ]\n",
    "\n",
    "    # Order the labels by start time\n",
    "    # If two events start at the same time, order by class index\n",
    "    merged_labels = sorted(\n",
    "        merged_labels, key=lambda x: (x[1], labels_.index(x[0]))\n",
    "    )\n",
    "\n",
    "    return merged_labels\n",
    "\n",
    "\n",
    "print(process_output(target[0]), process_output(prediction[0]), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.round(prediction[0], decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_melspectrogram(urbansed_val.audios[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_test_set():\n",
    "    urbansed_test = UrbanSEDDataset(\n",
    "                audios=[\n",
    "                    audioclip\n",
    "                    for _, audio in enumerate(\n",
    "                        AudioFile(\n",
    "                            filepath=file.filepath, labels=eval(file.events)\n",
    "                        )\n",
    "                        for _, file in pd.read_csv(\n",
    "                            \"./data/raw/URBAN-SED/test.csv\"\n",
    "                        ).iterrows()\n",
    "                    )\n",
    "                    for audioclip in audio.subdivide(\n",
    "                        win_len=2.56, hop_len=1.00\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "    return urbansed_test\n",
    "\n",
    "urbansed_test = get_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbansed_test_dataloader = YOHODataGenerator(\n",
    "    urbansed_test, batch_size=1, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth for audio clip 0: [('noise', 0, 2.56), ('siren', 1.896800669134819, 2.56)]\n",
      "Prediction for audio clip 0: [('noise', 0.0, 2.56), ('street_music', 1.88, 2.56)]\n",
      "F1: 0.600, Error: 0.400\n"
     ]
    }
   ],
   "source": [
    "import sed_eval\n",
    "import dcase_util\n",
    "\n",
    "labels_ = [\n",
    "    \"noise\",\n",
    "    \"air_conditioner\",\n",
    "    \"car_horn\",\n",
    "    \"children_playing\",\n",
    "    \"dog_bark\",\n",
    "    \"drilling\",\n",
    "    \"engine_idling\",\n",
    "    \"gun_shot\",\n",
    "    \"jackhammer\",\n",
    "    \"siren\",\n",
    "    \"street_music\",\n",
    "]\n",
    "\n",
    "all_thruth_data = dcase_util.containers.MetaDataContainer()\n",
    "all_predictions = dcase_util.containers.MetaDataContainer()\n",
    "\n",
    "\n",
    "for idx, (features, _) in enumerate(urbansed_test_dataloader):\n",
    "\n",
    "    # Get model predictions\n",
    "    prediction = urbansed_model(features.to(device))\n",
    "    labels = process_output(prediction[0])\n",
    "\n",
    "    thruth = urbansed_test.audios[idx].labels\n",
    "    print(f\"Truth for audio clip {idx}: {thruth}\")\n",
    "    print(f\"Prediction for audio clip {idx}: {labels}\")\n",
    "\n",
    "    all_thruth_data += dcase_util.containers.MetaDataContainer(\n",
    "        [\n",
    "            {\n",
    "                \"filename\": f\"{idx}.wav\",\n",
    "                \"onset\": onset,\n",
    "                \"offset\": offset,\n",
    "                \"event_label\": label,\n",
    "            }\n",
    "            for label, onset, offset in labels\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    all_predictions = dcase_util.containers.MetaDataContainer(\n",
    "        [\n",
    "            {\n",
    "                \"filename\": f\"{idx}.wav\",\n",
    "                \"onset\": onset,\n",
    "                \"offset\": offset,\n",
    "                \"event_label\": label,\n",
    "            }\n",
    "            for label, onset, offset in thruth\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
    "    event_label_list=labels_,\n",
    "    time_resolution=1.0,\n",
    ")\n",
    "\n",
    "# Evaluate the predictions\n",
    "segment_based_metrics.evaluate(all_predictions, all_thruth_data)\n",
    "\n",
    "overall_segment_based_metrics = segment_based_metrics.results_overall_metrics()\n",
    "curr_f1 = overall_segment_based_metrics['f_measure']['f_measure']\n",
    "curr_error = overall_segment_based_metrics['error_rate']['error_rate']\n",
    "\n",
    "print(\"F1: {:.3f}, Error: {:.3f}\".format(curr_f1, curr_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
