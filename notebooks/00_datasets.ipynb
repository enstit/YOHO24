{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOHO Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to locally download the datasets used in the [the original YOHO paper](https://doi.org/10.48550/arXiv.2109.00962), meaning:\n",
    "\n",
    "* the _MuSpeak dataset_,\n",
    "* the _TUT Sound Event Detection dataset_ and\n",
    "* the _Urban Sound Event Detection dataset_.\n",
    "\n",
    "We defined useful funcitons in the `utils` package to make easier the process of data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as audio_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music-Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MuSpeak Dataset\n",
    "musp_url = \"https://mirg.city.ac.uk/datasets/muspeak/muspeak-mirex2015-detection-examples.zip\"\n",
    "musp_zip_path = \"../data/musp.zip\"\n",
    "musp_extract_to = \"../data/musp\"\n",
    "\n",
    "audio_utils.download_file(musp_url, musp_zip_path)\n",
    "audio_utils.uncompress_file(musp_zip_path, musp_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUT Sound Events Detection\n",
    "\n",
    "The dataset is downloaded from the [DCASE2017 Challenge official website](https://dcase.community/challenge2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TUT Sound Events 2017 Dataset\n",
    "tut_urls = [(\"TUT-sound-events-2017-development\", \"https://zenodo.org/api/records/814831/files-archive\"),\n",
    "            (\"TUT-sound-events-2017-evaluation\", \"https://zenodo.org/api/records/1040179/files-archive\")\n",
    "            ]\n",
    "tut_zip_path = \"../data/tut.zip\"\n",
    "tut_extract_to = \"../data/tut\"\n",
    "\n",
    "for tut_name, tut_url in tut_urls:\n",
    "    tut_extract_to_subfolder = tut_extract_to + '/' + tut_name\n",
    "    audio_utils.download_file(tut_url, tut_zip_path)\n",
    "    audio_utils.uncompress_file(tut_zip_path, tut_extract_to_subfolder)\n",
    "\n",
    "    for item in tqdm(os.listdir(tut_extract_to_subfolder)):\n",
    "        if item.endswith('.zip'):\n",
    "            zipped_file = tut_extract_to_subfolder + '/' + item\n",
    "            unzipped_file = zipped_file.rsplit(\n",
    "                \".\", 1)[0]  # Remove .zip extension\n",
    "            audio_utils.uncompress_file(zipped_file, unzipped_file)\n",
    "            os.remove(zipped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Urban-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urban-SED Dataset\n",
    "urbansed_url = \"https://zenodo.org/api/records/1324404/files-archive\"\n",
    "urbansed_zip_path = \"../data/urbansed.zip\"\n",
    "urbansed_extract_to = \"../data/urbansed\"\n",
    "\n",
    "audio_utils.download_file(urbansed_url, urbansed_zip_path)\n",
    "audio_utils.uncompress_file(urbansed_zip_path, urbansed_extract_to)\n",
    "# The folder contains a compressed subfolder\n",
    "audio_utils.uncompress_file(urbansed_extract_to + \"/URBAN-SED_v2.0.0.tar.gz\", urbansed_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Process Annotations**: Read the annotation files for each dataset and convert the event data into a unified format. For example, for the MUSP dataset, annotations are transformed into a list of events where each event is represented as a tuple containing the event type ('m' for music or 's' for speech), start time, and end time.\n",
    "\n",
    "2. **Save Processed Data**: Save the processed data, including the audio file paths and their corresponding events, into a new CSV file. This structured data will serve as the input for data generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music-Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "MUSP_ANNOTATIONS_PATH = './data/musp/'\n",
    "\n",
    "# Get all the annotation files (name) in the data path:\n",
    "files = audio_utils.get_files(MUSP_ANNOTATIONS_PATH, extensions='.csv')\n",
    "\n",
    "# !! Remove some annotation files that are not linked to audio files:\n",
    "files.remove('theconcert2_v2.csv')  # no file named 'theconcert2_v2.mp3'\n",
    "files.remove('UTMA-26_v2.csv')      # no file named 'UTMA-26_v2.mp3'\n",
    "\n",
    "musp_data = {}  # This dictionary will store for each audio file the list of events\n",
    "                # i.e. {'./data/musp/audio_file.mp3': [('s', 20, 22), ('s', 25, 27), ...]}\n",
    "\n",
    "for f in files:\n",
    "    with open(MUSP_ANNOTATIONS_PATH + f, 'r') as file:\n",
    "\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        MP3_FILE_PATH = './data/musp/' + f.split('.')[0] + '.mp3'\n",
    "        musp_data[MP3_FILE_PATH] = []       # Initialize the list of events for this file\n",
    "\n",
    "        print(f'Processing {MP3_FILE_PATH}...')\n",
    "\n",
    "        for row in reader: # Read the events from the CSV file\n",
    "            \n",
    "            if not row: # Skip empty lines\n",
    "              continue\n",
    "  \n",
    "            start = float(row[0])       # Start time of the event (in seconds)\n",
    "            duration = float(row[1])    # Duration of the event (in seconds)\n",
    "            end_time = start + duration # End time of the event (in seconds)\n",
    "            label = str(row[2])         # Label of the event  \n",
    "\n",
    "            assert (start < end_time), f\"Start time ({start}) must be less than end time ({end_time})\"\n",
    "            # Append the event to the list of events for this audio file:\n",
    "            musp_data[MP3_FILE_PATH].append(\n",
    "              (label, start, end_time) \n",
    "            )                          \n",
    "\n",
    "audio_utils.write_data_to_csv(musp_data, './data/musp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUT Sound Events Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_1_PATH = './data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.audio.1/TUT-sound-events-2017-development/audio/street/'\n",
    "AUDIO_2_PATH = './data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.audio.2/TUT-sound-events-2017-development/audio/street/'\n",
    "DATA_PATH = './data/tut/'\n",
    "\n",
    "DEVELOPMENT_ANNOTATIONS_PATH = './data/tut/TUT-sound-events-2017-development/TUT-sound-events-2017-development.meta/TUT-sound-events-2017-development/meta/street/'\n",
    "\n",
    "files = audio_utils.get_files(DEVELOPMENT_ANNOTATIONS_PATH, extensions='.ann')\n",
    "\n",
    "tut_data = {}\n",
    "for f in files:\n",
    "    with open(DEVELOPMENT_ANNOTATIONS_PATH + f, 'r'):\n",
    "\n",
    "        f_name = f.split('.')[0] + '.wav'\n",
    "\n",
    "        if f_name in ['a128.wav', 'a131.wav', 'b007.wav', 'b093.wav']:\n",
    "            f_path = AUDIO_2_PATH + f_name\n",
    "        else:\n",
    "          f_path = AUDIO_1_PATH + f_name\n",
    "          \n",
    "        tut_data[f_path] = []\n",
    "\n",
    "        print(f'Processing {f_path}...')\n",
    "\n",
    "        with open(DEVELOPMENT_ANNOTATIONS_PATH + f, 'r') as file:\n",
    "\n",
    "            reader = csv.reader(file)\n",
    "\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    # split in \\t and get the start and end time\n",
    "                    row = row[0].split('\\t')\n",
    "                    start = float(row[2])\n",
    "                    end = float(row[3])\n",
    "                    label = row[4]\n",
    "                    tut_data[f_path].append(\n",
    "                        (label, start, end)\n",
    "                    )\n",
    "\n",
    "audio_utils.write_data_to_csv(tut_data, './data/tut.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Urban-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jams\n",
    "URBAN_SED_ANNOTATIONS_PATH = './data/urbansed/annotations/train/'\n",
    "\n",
    "files = audio_utils.get_files(URBAN_SED_ANNOTATIONS_PATH, extensions='jams')\n",
    "\n",
    "def parse_jams_file(jams_file):\n",
    "    \"\"\"\n",
    "    Parse a JAMS file and extract the annotations.\n",
    "\n",
    "    Parameters:\n",
    "    - jams_file (str): The path to the JAMS file.\n",
    "\n",
    "    Returns:\n",
    "    - events (list): A list of tuples containing the event type, start time, and end time.\n",
    "    \"\"\"\n",
    "    jam = jams.load(jams_file)\n",
    "    events = []\n",
    "    for annotation in jam.annotations:\n",
    "\n",
    "        for obs in annotation.data:\n",
    "\n",
    "            start_time = obs.value['event_time']\n",
    "            end_time = obs.value['event_time'] + obs.value['event_duration']\n",
    "            label = obs.value['label']\n",
    "            events.append((label, start_time, end_time))\n",
    "\n",
    "        return events\n",
    "\n",
    "DATA_PATH = './data/urbansed/annotations/train/'\n",
    "AUDIO_PATH = './data/urbansed/audio/train/'\n",
    "\n",
    "urbansed_data = {}\n",
    "for f in files:\n",
    "    f_path = DATA_PATH + f\n",
    "    f = f.split('.')[0] + '.wav'\n",
    "    urbansed_data[AUDIO_PATH+f] = parse_jams_file(f_path)\n",
    "\n",
    "audio_utils.write_data_to_csv(urbansed_data, './data/urbansed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
